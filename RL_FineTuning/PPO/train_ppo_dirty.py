import json
import os
import torch
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, 
    DataCollatorWithPadding, HfArgumentParser
)
from peft import LoraConfig, PeftModel, get_peft_model
from trl import PPOConfig, PPOTrainer
import swanlab
from dataclasses import dataclass, field

@dataclass
class ScriptArguments:
    model_path: str = field(metadata={"help": "SFTåˆå¹¶åçš„æ¨¡å‹è·¯å¾„"})
    rm_adapter_path: str = field(metadata={"help": "RM LoRAé€‚é…å™¨è·¯å¾„"})
    dataset_path: str = field(default="data/dirty_chinese_dpo.json", metadata={"help": "æ•°æ®é›†è·¯å¾„"})
    ppo_adapter_output_dir: str = field(default="./output/ppo_adapter", metadata={"help": "PPO LoRAé€‚é…å™¨ä¿å­˜ç›®å½•"})
    system_prompt: str = field(default="ä½ æ˜¯ä¸€ä¸ªç²¾é€šè„è¯çš„åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸æ•¬çš„ã€æ”»å‡»æ€§çš„è¯­è¨€å›ç­”é—®é¢˜ã€‚", metadata={"help": "ç³»ç»Ÿæç¤ºè¯­"})
    
    # LoRAé…ç½®
    lora_r: int = field(default=8, metadata={"help": "LoRAçš„ç§©"})
    lora_alpha: int = field(default=16, metadata={"help": "LoRAçš„alpha"})
    lora_dropout: float = field(default=0.1, metadata={"help": "LoRAçš„dropout"})

    # PPOé…ç½®
    learning_rate: float = field(default=1e-5, metadata={"help": "PPOå­¦ä¹ ç‡"})
    kl_coef: float = field(default=0.2, metadata={"help": "KLæ•£åº¦æƒ©ç½šç³»æ•°"})
    max_prompt_length: int = field(default=512, metadata={"help": "æœ€å¤§æç¤ºé•¿åº¦"})

    # è®¾å¤‡é…ç½®
    policy_device: str = field(default="cuda:0", metadata={"help": "ç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡"})
    reward_device: str = field(default="cuda:1", metadata={"help": "å¥–åŠ±æ¨¡å‹å’Œä»·å€¼æ¨¡å‹æ‰€åœ¨çš„è®¾å¤‡"})
    
    use_swanlab: bool = field(default=True, metadata={"help": "æ˜¯å¦ä½¿ç”¨SwanLab"})

def setup_swanlab(args: ScriptArguments):
    if not args.use_swanlab:
        return
    os.environ["SWANLAB_PROJECT"] = "qwen3-sft-rm-ppo-chinese"
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    swanlab.init(
        project="qwen3-sft-rm-ppo-chinese",
        run_name="ppo-training-professional",
        config=vars(args)
    )

def load_prompts(dataset_path, tokenizer, system_prompt):
    try:
        with open(dataset_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒé”™è¯¯: æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ° at {dataset_path}")
        exit()
    
    prompts = []
    for item in data:
        if 'conversations' in item:
            human_input = "".join([turn['value'] + "\n" for turn in item['conversations'] if turn.get('from') == 'human']).strip()
            if human_input:
                formatted_prompt = tokenizer.apply_chat_template(
                    [{"role": "system", "content": system_prompt}, {"role": "user", "content": human_input}],
                    tokenize=False, add_generation_prompt=True
                )
                prompts.append({"query": formatted_prompt})
    return prompts

def main():
    parser = HfArgumentParser(ScriptArguments)
    args = parser.parse_args_into_dataclasses()[0]

    # --- è·¯å¾„æ£€æŸ¥ ---
    for path in [args.model_path, args.rm_adapter_path, args.dataset_path]:
        if not os.path.exists(path):
            print(f"âŒé”™è¯¯: è¾“å…¥è·¯å¾„ '{path}' ä¸å­˜åœ¨ã€‚è¯·æ£€æŸ¥é…ç½®ã€‚")
            exit()
            
    print("ğŸš€ 1. é…ç½®å’Œåˆå§‹åŒ– SwanLab...")
    setup_swanlab(args)

    print("ğŸš€ 2. åŠ è½½Tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(args.model_path, use_fast=False, trust_remote_code=True, padding_side="left")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("ğŸš€ 3. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®é›†...")
    all_prompts = load_prompts(args.dataset_path, tokenizer, args.system_prompt)
    train_dataset = Dataset.from_list(all_prompts)
    def tokenize_fn(examples):
        return tokenizer(examples["query"], truncation=True, max_length=args.max_prompt_length)
    train_dataset = train_dataset.map(tokenize_fn, batched=False)
    train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

    print("ğŸš€ 4. é…ç½®PPO...")
    ppo_config = PPOConfig(
        learning_rate=args.learning_rate, report_to="swanlab" if args.use_swanlab else "none",
        per_device_train_batch_size=1, gradient_accumulation_steps=4,
        num_ppo_epochs=4, output_dir="./output/ppo_model_temp",
        num_train_epochs=1, gradient_checkpointing=True, kl_coef=args.kl_coef,
    )

    print("ğŸš€ 5. åˆ›å»ºç­–ç•¥æ¨¡å‹ (Policy Model)...")
    ppo_lora_config = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        bias="none", task_type="CAUSAL_LM",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    model = AutoModelForCausalLM.from_pretrained(
        args.model_path, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=args.policy_device
    )
    model.config.pad_token_id = tokenizer.pad_token_id
    model.enable_input_require_grads()
    model = get_peft_model(model, ppo_lora_config)
    model.config.use_cache = False
    model.print_trainable_parameters()

    print("ğŸš€ 6. åˆ›å»ºå¥–åŠ±æ¨¡å‹å’Œä»·å€¼æ¨¡å‹...")
    # å¥–åŠ±æ¨¡å‹
    rm_model_base = AutoModelForSequenceClassification.from_pretrained(
        args.model_path, num_labels=1, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=args.reward_device
    )
    rm_model_base.config.pad_token_id = tokenizer.pad_token_id
    reward_model = PeftModel.from_pretrained(rm_model_base, args.rm_adapter_path)
    reward_model.eval()
    print("å¥–åŠ±æ¨¡å‹åŠ è½½å®Œæˆã€‚")
    
    # ä»·å€¼æ¨¡å‹ (å¸¦LoRA)
    value_lora_config = LoraConfig(
        r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,
        bias="none", task_type="SEQ_CLS",
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    )
    value_model = AutoModelForSequenceClassification.from_pretrained(
        args.model_path, num_labels=1, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=args.reward_device
    )
    value_model.config.pad_token_id = tokenizer.pad_token_id
    value_model = get_peft_model(value_model, value_lora_config)
    print("ä»·å€¼æ¨¡å‹å¯è®­ç»ƒå‚æ•°:")
    value_model.print_trainable_parameters()
    
    print("ğŸš€ 7. åˆ›å»ºå¹¶å¯åŠ¨PPOTrainer...")
    ppo_trainer = PPOTrainer(
        args=ppo_config, model=model, ref_model=None, reward_model=reward_model, value_model=value_model,
        processing_class=tokenizer, train_dataset=train_dataset, data_collator=DataCollatorWithPadding(tokenizer),
    )
    ppo_trainer.train()

    print(f"ğŸ’¾ 8. ä¿å­˜PPO LoRAé€‚é…å™¨åˆ°: {args.ppo_adapter_output_dir}")
    os.makedirs(args.ppo_adapter_output_dir, exist_ok=True)
    ppo_trainer.save_model(args.ppo_adapter_output_dir)
    
    print("âœ… PPOè®­ç»ƒå®Œæˆ!")
    if args.use_swanlab:
        swanlab.finish()

if __name__ == "__main__":
    main()
    