import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# --- é…ç½® ---
BASE_MODEL_PATH = "/home/space/space/model/Qwen3-1.7B"
SFT_ADAPTER_PATH = "./output/sft_adapter"
MERGED_MODEL_OUTPUT_PATH = "./output/sft_merged_model"

def main():
    """
    è¯¥è„šæœ¬ç”¨äºå°†SFTé˜¶æ®µè®­ç»ƒå¥½çš„LoRAé€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼Œ
    å¹¶å°†å…¶ä¿å­˜ä¸ºä¸€ä¸ªç‹¬ç«‹çš„æ¨¡å‹ï¼Œä»¥ä¾›åç»­çš„RMå’ŒPPOé˜¶æ®µä½¿ç”¨ã€‚
    """
    print("ğŸš€ å¼€å§‹ SFT é€‚é…å™¨åˆå¹¶...")

    if not os.path.exists(SFT_ADAPTER_PATH):
        raise FileNotFoundError(f"SFT é€‚é…å™¨æœªåœ¨ {SFT_ADAPTER_PATH} æ‰¾åˆ°ã€‚è¯·å…ˆè¿è¡Œ train_sft.pyã€‚")

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹
    print(f"æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH}")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="cpu", # åœ¨CPUä¸ŠåŠ è½½ä»¥é¿å…åˆå¹¶æ—¶æ˜¾å­˜ä¸è¶³
        trust_remote_code=True,
    )

    # 2. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    # 3. åŠ è½½ SFT LoRA é€‚é…å™¨
    print(f"æ­£åœ¨ä»ä»¥ä¸‹è·¯å¾„åŠ è½½ SFT é€‚é…å™¨: {SFT_ADAPTER_PATH}")
    model_to_merge = PeftModel.from_pretrained(base_model, SFT_ADAPTER_PATH)

    # 4. è°ƒç”¨ merge_and_unload å°†é€‚é…å™¨æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹
    print("æ­£åœ¨å°†é€‚é…å™¨åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­...")
    merged_model = model_to_merge.merge_and_unload()
    print("åˆå¹¶å®Œæˆã€‚")

    # 5. ä¿å­˜åˆå¹¶åçš„æ¨¡å‹å’Œ Tokenizer
    print(f"æ­£åœ¨å°†åˆå¹¶åçš„æ¨¡å‹ä¿å­˜åˆ°: {MERGED_MODEL_OUTPUT_PATH}")
    os.makedirs(MERGED_MODEL_OUTPUT_PATH, exist_ok=True)
    merged_model.save_pretrained(MERGED_MODEL_OUTPUT_PATH)
    tokenizer.save_pretrained(MERGED_MODEL_OUTPUT_PATH)
    
    print("âœ… åˆå¹¶åçš„æ¨¡å‹å·²æˆåŠŸä¿å­˜ï¼")

if __name__ == "__main__":
    main()