# LLM_fine_turning/inference_psydt_lora.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse
import os
import json
from typing import List, Dict

DEFAULT_SYSTEM_PROMPT = (
    "ä½ æ˜¯ä¸€åä¸“ä¸šã€å…±æƒ…çš„å¿ƒç†å’¨è¯¢åŠ©ç†ã€‚ä½ çš„å›å¤åº”ï¼š"
    "1) å…±æƒ…ä¸ç†è§£ï¼›2) å…³æ³¨æ¥è®¿è€…çš„æƒ…ç»ªä¸éœ€è¦ï¼›"
    "3) é¿å…è¯Šæ–­ä¸è¯„åˆ¤ï¼›4) æä¾›æ”¯æŒæ€§å»ºè®®ä¸å¯æ‰§è¡Œçš„å°æ­¥éª¤ï¼›"
    "5) é¼“åŠ±åœ¨éœ€è¦æ—¶å¯»æ±‚ä¸“ä¸šå¸®åŠ©ã€‚"
)

class PsyDTChatbot:
    def __init__(
        self,
        base_model_path: str,
        lora_adapter_path: str,
        system_prompt: str = DEFAULT_SYSTEM_PROMPT,
        history_turns: int = 6
    ):
        self.base_model_path = base_model_path
        self.lora_adapter_path = lora_adapter_path
        self.system_prompt = system_prompt
        self.history_turns = max(0, history_turns)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.tokenizer = None
        self.model = None
        self.history: List[Dict[str, str]] = []
        self.reset()

    def reset(self):
        self.history = [{"role": "system", "content": self.system_prompt}]

    def load_model(self, merge_lora: bool = False):
        print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹ ...")
        print(f"--> åŸºåº§æ¨¡å‹: {self.base_model_path}")
        print(f"--> LoRA é€‚é…å™¨: {self.lora_adapter_path}")

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.base_model_path, use_fast=False, trust_remote_code=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            device_map="auto",
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            trust_remote_code=True
        )
        self.model = PeftModel.from_pretrained(self.model, model_id=self.lora_adapter_path)

        if merge_lora:
            self.model = self.model.merge_and_unload()

        self.model.eval()
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œè®¾å¤‡: {self.model.device}")

    def _trim_history(self):
        # ä»…ä¿ç•™æœ€è¿‘ N è½® user/assistantï¼ˆä¸å« systemï¼‰
        if self.history_turns <= 0:
            return
        sys = self.history[0] if self.history and self.history[0]["role"] == "system" else None
        msgs = self.history[1:] if sys else self.history[:]
        # æ¯è½®åŒ…å« user+assistant ä¸¤æ¡ï¼›æŒ‰æœ«å°¾æˆªå–
        kept = []
        ua = 0
        for m in reversed(msgs):
            kept.append(m)
            if m["role"] == "assistant":
                ua += 1
                if ua >= self.history_turns:
                    break
        kept = list(reversed(kept))
        self.history = ([sys] if sys else []) + kept

    @torch.no_grad()
    def chat(
        self,
        user_text: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        repetition_penalty: float = 1.1
    ) -> str:
        self.history.append({"role": "user", "content": user_text})
        self._trim_history()

        prompt_text = self.tokenizer.apply_chat_template(
            self.history, tokenize=False, add_generation_prompt=True
        )
        inputs = self.tokenizer([prompt_text], return_tensors="pt").to(self.model.device)

        output_ids = self.model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True,
            repetition_penalty=repetition_penalty,
            pad_token_id=self.tokenizer.pad_token_id,
        )
        gen_ids = output_ids[0][len(inputs.input_ids[0]):]
        response = self.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()

        self.history.append({"role": "assistant", "content": response})
        return response


def run_interactive(bot: PsyDTChatbot, args):
    print("\n" + "="*80)
    print("ğŸ¯ å¿ƒç†å’¨è¯¢å¤šè½®å¯¹è¯ - äº¤äº’æ¨¡å¼\nè¾“å…¥ 'exit' æˆ– 'quit' é€€å‡ºï¼Œä¼šè¯ä½¿ç”¨å¤šè½®è®°å¿†ã€‚")
    print("="*80)

    while True:
        try:
            user_input = input("\nğŸ‘¤ æ¥è®¿è€…: ").strip()
            if user_input.lower() in ["exit", "quit"]:
                break
            if not user_input:
                continue
            print("ğŸ¤– å’¨è¯¢åŠ©ç†: ", end="", flush=True)
            reply = bot.chat(
                user_input,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                repetition_penalty=args.repetition_penalty
            )
            print(reply)
        except (KeyboardInterrupt, EOFError):
            break
    print("\nğŸ‘‹ å·²ç»“æŸå¯¹è¯")

def run_test(bot: PsyDTChatbot, args):
    test_questions = [
        "æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯ç¡ä¸å¥½ï¼Œæ€ä¹ˆåŠï¼Ÿ",
        "æˆ‘ç»å¸¸å¯¹è‡ªå·±å¾ˆè‹›åˆ»ï¼Œè§‰å¾—è‡ªå·±ä¸å¤Ÿå¥½ã€‚",
        "å’Œå®¶äººçš„æ²Ÿé€šæ€»æ˜¯ä¼šåµèµ·æ¥ï¼Œæˆ‘ä¸æƒ³è¿™æ ·ã€‚",
        "æˆ‘æœ‰ç‚¹ç„¦è™‘ï¼Œå®³æ€•åœ¨å…¬ä¼—åœºåˆè¡¨è¾¾ã€‚",
        "èƒ½ç»™æˆ‘ä¸€äº›ç¼“è§£ç„¦è™‘çš„å°ç»ƒä¹ å—ï¼Ÿ"
    ]
    results = []
    print("\n" + "="*80)
    print("ğŸ§ª æ‰¹é‡æµ‹è¯•å¼€å§‹")
    print("="*80)
    for i, q in enumerate(test_questions, 1):
        print(f"\nğŸ“ {i}/{len(test_questions)} é¢˜ç›®: {q}\n" + "-"*60)
        resp = bot.chat(
            q,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            repetition_penalty=args.repetition_penalty
        )
        print(f"ğŸ¤– å›å¤: {resp}")
        results.append({"question": q, "response": resp})

    if args.test_output_file:
        with open(args.test_output_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜è‡³: {args.test_output_file}")

def main():
    parser = argparse.ArgumentParser(description="PsyDT LoRA å¤šè½®å¯¹è¯æ¨ç†è„šæœ¬")
    parser.add_argument("--model_path", type=str, required=True, help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¦‚ ./Qwen/Qwen3-1.7B")
    parser.add_argument("--adapter_path", type=str, required=True, help="LoRA é€‚é…å™¨è·¯å¾„ï¼Œå¦‚ ./output/qwen3-1_7b-psydt-lora/lora_adapter")
    parser.add_argument("--mode", type=str, default="interactive", choices=["interactive", "test"])
    parser.add_argument("--system_prompt", type=str, default=DEFAULT_SYSTEM_PROMPT)
    parser.add_argument("--history_turns", type=int, default=6, help="ä¿ç•™æœ€è¿‘ N è½®å¯¹è¯è®°å¿†")
    parser.add_argument("--merge_lora", action="store_true", help="å°† LoRA åˆå¹¶è¿›åŸºåº§æ¨¡å‹ä»¥åŠ é€Ÿæ¨ç†")
    parser.add_argument("--max_new_tokens", type=int, default=256)
    parser.add_argument("--temperature", type=float, default=0.7)
    parser.add_argument("--top_p", type=float, default=0.9)
    parser.add_argument("--repetition_penalty", type=float, default=1.1)
    parser.add_argument("--test_output_file", type=str, default="psydt_sft_test_results.json")
    args = parser.parse_args()

    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
    if not os.path.exists(args.adapter_path):
        raise FileNotFoundError(f"LoRA é€‚é…å™¨è·¯å¾„ä¸å­˜åœ¨: {args.adapter_path}")

    bot = PsyDTChatbot(
        base_model_path=args.model_path,
        lora_adapter_path=args.adapter_path,
        system_prompt=args.system_prompt,
        history_turns=args.history_turns
    )
    bot.load_model(merge_lora=args.merge_lora)

    if args.mode == "interactive":
        run_interactive(bot, args)
    else:
        run_test(bot, args)

if __name__ == "__main__":
    main()